# -*- coding: utf-8 -*-
"""str_no_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KhJ2SBh8-lUuYEfTdCZg_MlWDB3RYkJS
"""

#Really need thesey
import pandas as pd
import numpy as np
from numpy import *

#Nice graphing tools
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import plotly
import plotly.offline as py
import plotly.tools as tls
import plotly.graph_objs as go
import plotly.tools as tls
from plotly.subplots import make_subplots

#Machine learning tools
from sklearn.model_selection import KFold,GridSearchCV,train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error, make_scorer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_percentage_error

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
# Fitting Linear Regression to the dataset
from sklearn.linear_model import LinearRegression

# # Assuming your dataset is in a DataFrame named 'df' with columns 'Year', 'Month', 'Sales'
df = pd.read_csv('C:/Users/Owner/Documents/Python Project/python API/sales_&_store.csv')
test = pd.read_csv('C:/Users/Owner/Documents/Python Project/python API/101stores.csv')
df.head()

#Checking the columns of the dataset
df.columns

df.info()

df.describe()

df.nunique().sort_values()

"""# Data Cleaning"""

# Checking null values
clean_df = df.copy()
clean_df.isnull().sum()*100/clean_df.shape[0]

#Removal of any Duplicate rows (if any)

counter = 0
rs,cs = clean_df.shape

clean_df.drop_duplicates(inplace=True)
test.drop_duplicates(inplace=True)
if clean_df.shape==(rs,cs):
    print('\n\033[1mInference:\033[0m The dataset doesn\'t have any duplicates')
else:
    print(f'\n\033[1mInference:\033[0m Number of duplicates dropped/fixed ---> {rs-clean_df.shape[0]}')

# Formatting for sales_date and str_no

clean_df['store'] = clean_df['store'].astype(str)

clean_df['date'] = pd.to_datetime(clean_df['date'])

print(clean_df['store'].dtypes,clean_df['date'].dtypes)

test['store'] = test['store'].astype(str)

test['date'] = pd.to_datetime(test['date'])

print(test['store'].dtypes,test['date'].dtypes)

#TARGET ENCODING

target_encoding = clean_df.groupby('store')['total_sales'].mean()
clean_df['store'] = clean_df['store'].map(target_encoding)
target_encoding = test.groupby('store')['total_sales'].mean()
test['store'] = test['store'].map(target_encoding)
clean_df.head()

"""# DATA EXPLORATION

"""

exp_df = clean_df.copy()
df.head()

# FEATURE ENGINEERING
# day/time features

# Assuming 'sales_date' is in datetime format
exp_df['day_of_week'] = exp_df['date'].dt.dayofweek
exp_df['month'] = exp_df['date'].dt.month
exp_df['quarter'] = exp_df['date'].dt.quarter
exp_df['year'] = exp_df['date'].dt.year

test['day_of_week'] = test['date'].dt.dayofweek
test['month'] = test['date'].dt.month
test['quarter'] = test['date'].dt.quarter
test['year'] = test['date'].dt.year
test.head()

"""# DATA SPLITTING"""

split_df = exp_df.copy()

split_df.head()

# Split the data into features (X) and target variable (y)
X = split_df[['store','day_of_week','month','quarter','year']]
# 'sales_date','day_of_week','month','quarter','year'
y = split_df['total_sales']
# 'sales_date','day_of_week','month','quarter','year'
# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, shuffle=False)

# Test Split
X_test = test[['store','day_of_week','month','quarter','year']]
y_test = test['total_sales']

# Display the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("X_dev shape:", X_dev.shape)
print("y_train shape:", y_train.shape)
print("y_dev shape:", y_dev.shape)

"""# MODEL TRAINING

XGBoost
"""

import xgboost as xgb

xg_reg = xgb.XGBRegressor()
parameters = {
        'n_estimators': [10, 50, 100],
        'loss':['huber'],
        'criterion': ['friedman_mse', 'squared_error'],
        'max_depth': [2, 3, 4],
        'learning_rate': [0.01, 0.1, 1],
        'min_samples_split': [2, 4, 6],
        'min_samples_leaf': [1, 2, 3]
}

# Create the grid search object
grid_search = GridSearchCV(estimator=xg_reg, param_grid=parameters, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Predict on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Best Hyperparameters: {best_params}')
print('MAPE score using Gradient Boosting= ',mean_absolute_percentage_error(y_test, y_pred))
print('RMSE score using Gradient Boosting= ',mean_squared_error(y_test, y_pred))

import pickle

with open('XGBoost_storemodel.pkl','wb') as file:
    pickle.dump(best_model, file)

"""Polynomial Regression"""

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train)

lin2 = LinearRegression()
lin2.fit(X_poly, y_train)

y_pred = lin2.predict(poly.fit_transform(X_test))

print('MAPE score using Gradient Boosting= ',mean_absolute_percentage_error(y_test, y_pred))
print('RMSE score using Gradient Boosting= ',mean_squared_error(y_test, y_pred))

import pickle

with open('poly_storemodel.pkl','wb') as file:
    pickle.dump(lin2, file)