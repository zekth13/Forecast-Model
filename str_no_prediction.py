# -*- coding: utf-8 -*-
"""str_no_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KhJ2SBh8-lUuYEfTdCZg_MlWDB3RYkJS
"""

#Really need thesey
import pandas as pd
import numpy as np
from numpy import *

#Machine learning tools
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import train_test_split

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
import connector

def monthstoreprediction(val):
    # # Assuming your dataset is in a DataFrame named 'df' with columns 'Year', 'Month', 'Sales'
    df = connector.storedata()
    test = connector.singlestoredata(val)
    df.head()

    #Checking the columns of the dataset
    df.columns

    df.info()

    df.describe()

    df.nunique().sort_values()

    """# Data Cleaning"""

    # Checking null values
    clean_df = df.copy()
    clean_df.isnull().sum()*100/clean_df.shape[0]

    #Removal of any Duplicate rows (if any)

    counter = 0
    rs,cs = clean_df.shape

    clean_df.drop_duplicates(inplace=True)
    test.drop_duplicates(inplace=True)
    if clean_df.shape==(rs,cs):
        print('\n\033[1mInference:\033[0m The dataset doesn\'t have any duplicates')
    else:
        print(f'\n\033[1mInference:\033[0m Number of duplicates dropped/fixed ---> {rs-clean_df.shape[0]}')

    # Formatting for sales_date and str_no

    clean_df['store'] = clean_df['store'].astype(str)

    clean_df['date'] = pd.to_datetime(clean_df['date'])

    print(clean_df['store'].dtypes,clean_df['date'].dtypes)

    test['store'] = test['store'].astype(str)

    test['date'] = pd.to_datetime(test['date'])

    print(test['store'].dtypes,test['date'].dtypes)

    #TARGET ENCODING

    target_encoding = clean_df.groupby('store')['total_sales'].mean()
    clean_df['store'] = clean_df['store'].map(target_encoding)
    target_encoding = test.groupby('store')['total_sales'].mean()
    test['store'] = test['store'].map(target_encoding)
    clean_df.head()

    """# DATA EXPLORATION

    """

    exp_df = clean_df.copy()
    df.head()

    # FEATURE ENGINEERING
    # day/time features

    # Assuming 'sales_date' is in datetime format
    exp_df['day_of_week'] = exp_df['date'].dt.dayofweek
    exp_df['month'] = exp_df['date'].dt.month
    exp_df['quarter'] = exp_df['date'].dt.quarter
    exp_df['year'] = exp_df['date'].dt.year

    test['day_of_week'] = test['date'].dt.dayofweek
    test['month'] = test['date'].dt.month
    test['quarter'] = test['date'].dt.quarter
    test['year'] = test['date'].dt.year
    test.head()

    """# DATA SPLITTING"""

    split_df = exp_df.copy()

    split_df.head()

    # Split the data into features (X) and target variable (y)
    X = split_df[['store','day_of_week','month','quarter','year']]
    # 'sales_date','day_of_week','month','quarter','year'
    y = split_df['total_sales']
    # 'sales_date','day_of_week','month','quarter','year'
    # Split the data into training and testing sets (e.g., 80% train, 20% test)
    X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Test Split
    X_test = test[['store','day_of_week','month','quarter','year']]
    y_test = test['total_sales']
    # Display the shapes of the resulting sets
    print("X_train shape:", X_train.shape)
    print("X_dev shape:", X_dev.shape)
    print("y_train shape:", y_train.shape)
    print("y_dev shape:", y_dev.shape)

    """# Final Prediction"""

    """# XGBoost"""

    import pickle

    with open('XGBoost_storemodel.pkl', 'rb') as file:
        xgb_model = pickle.load(file)

    final_gbr_prediction = xgb_model.predict(X_test)

    xgb_mape = mean_absolute_percentage_error(y_test, final_gbr_prediction)
    xgb_rmse = mean_squared_error(y_test, final_gbr_prediction)
    xgb_result = [xgb_mape,xgb_rmse]

    testfile = pd.DataFrame(final_gbr_prediction)

    # Resetting the index of both DataFrames
    X_test.reset_index(drop=True, inplace=True)
    testfile.reset_index(drop=True, inplace=True)

    testfile.rename(columns={0: 'predicted_sales'}, inplace=True)

    # Assuming your DataFrame is named df and you want to round up values in the 'column_name' column
    testfile['predicted_sales'] = np.ceil(testfile['predicted_sales']).astype(int)

    # Combine the two datasets based on their index
    monthly_xgb_df = pd.concat([X_test, testfile], axis=1)

    # Group by 'year' and 'month' and sum 'predicted_sales'
    result_xgb_df = monthly_xgb_df.groupby(['year', 'month'], as_index=False)['predicted_sales'].sum()
    
    xgb_final = result_xgb_df.values.tolist()

    """# Polynomial Regression"""

    import pickle

    with open('poly_storemodel.pkl', 'rb') as file:
        poly_model = pickle.load(file)

    poly = PolynomialFeatures(degree=2)
    final_poly_prediction = poly_model.predict(poly.fit_transform(X_test))

    poly_mape = mean_absolute_percentage_error(y_test, final_poly_prediction)
    poly_rmse = mean_squared_error(y_test, final_poly_prediction)
    poly_result = [poly_mape,poly_rmse]

    testfile = pd.DataFrame(final_poly_prediction)

    # Resetting the index of both DataFrames
    X_test.reset_index(drop=True, inplace=True)
    testfile.reset_index(drop=True, inplace=True)

    testfile.rename(columns={0: 'predicted_sales'}, inplace=True)

    # Assuming your DataFrame is named df and you want to round up values in the 'column_name' column
    testfile['predicted_sales'] = np.ceil(testfile['predicted_sales']).astype(int)

    # Combine the two datasets based on their index
    monthly_poly_df = pd.concat([X_test, testfile], axis=1)

    # Group by 'year' and 'month' and sum 'predicted_sales'
    result_poly_df = monthly_poly_df.groupby(['year', 'month'], as_index=False)['predicted_sales'].sum()

    poly_final = result_poly_df.values.tolist()

    """Original Values"""

    split_df['total_sales'] = np.ceil(split_df['total_sales']).astype(int)
    monthly_df = split_df.groupby(['year', 'month'], as_index=False)['total_sales'].sum()
    monthly_final = monthly_df.values.tolist()

    return [xgb_final,poly_final,monthly_final,xgb_result,poly_result]


def yearstoreprediction(val):
    # # Assuming your dataset is in a DataFrame named 'df' with columns 'Year', 'Month', 'Sales'
    df = connector.storedata()
    test = connector.singlestoredata(val)
    df.head()

    #Checking the columns of the dataset
    df.columns

    df.info()

    df.describe()

    df.nunique().sort_values()

    """# Data Cleaning"""

    # Checking null values
    clean_df = df.copy()
    clean_df.isnull().sum()*100/clean_df.shape[0]

    #Removal of any Duplicate rows (if any)

    counter = 0
    rs,cs = clean_df.shape

    clean_df.drop_duplicates(inplace=True)
    test.drop_duplicates(inplace=True)
    if clean_df.shape==(rs,cs):
        print('\n\033[1mInference:\033[0m The dataset doesn\'t have any duplicates')
    else:
        print(f'\n\033[1mInference:\033[0m Number of duplicates dropped/fixed ---> {rs-clean_df.shape[0]}')

    # Formatting for sales_date and str_no

    clean_df['store'] = clean_df['store'].astype(str)

    clean_df['date'] = pd.to_datetime(clean_df['date'])

    print(clean_df['store'].dtypes,clean_df['date'].dtypes)

    test['store'] = test['store'].astype(str)

    test['date'] = pd.to_datetime(test['date'])

    print(test['store'].dtypes,test['date'].dtypes)

    #TARGET ENCODING

    target_encoding = clean_df.groupby('store')['total_sales'].mean()
    clean_df['store'] = clean_df['store'].map(target_encoding)
    target_encoding = test.groupby('store')['total_sales'].mean()
    test['store'] = test['store'].map(target_encoding)
    clean_df.head()

    """# DATA EXPLORATION

    """

    exp_df = clean_df.copy()
    df.head()

    # FEATURE ENGINEERING
    # day/time features

    # Assuming 'sales_date' is in datetime format
    exp_df['day_of_week'] = exp_df['date'].dt.dayofweek
    exp_df['month'] = exp_df['date'].dt.month
    exp_df['quarter'] = exp_df['date'].dt.quarter
    exp_df['year'] = exp_df['date'].dt.year

    test['day_of_week'] = test['date'].dt.dayofweek
    test['month'] = test['date'].dt.month
    test['quarter'] = test['date'].dt.quarter
    test['year'] = test['date'].dt.year
    test.head()

    """# DATA SPLITTING"""

    split_df = exp_df.copy()

    split_df.head()

    # Split the data into features (X) and target variable (y)
    X = split_df[['store','day_of_week','month','quarter','year']]
    # 'sales_date','day_of_week','month','quarter','year'
    y = split_df['total_sales']
    # 'sales_date','day_of_week','month','quarter','year'
    # Split the data into training and testing sets (e.g., 80% train, 20% test)
    X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Test Split
    X_test = test[['store','day_of_week','month','quarter','year']]
    y_test = test['total_sales']

    # Display the shapes of the resulting sets
    print("X_train shape:", X_train.shape)
    print("X_dev shape:", X_dev.shape)
    print("y_train shape:", y_train.shape)
    print("y_dev shape:", y_dev.shape)

    """# MODEL PREDICTION

    XGBoost
    """

    import pickle

    with open('XGBoost_storemodel.pkl', 'rb') as file:
        xgb_model = pickle.load(file)

    xgb_prediction = xgb_model.predict(X_test)

    xgb_mape = mean_absolute_percentage_error(y_test, xgb_prediction)
    xgb_rmse = mean_squared_error(y_test, xgb_prediction)
    xgb_result = [xgb_mape,xgb_rmse]

    testfile = pd.DataFrame(xgb_prediction)

    # Resetting the index of both DataFrames
    X_test.reset_index(drop=True, inplace=True)
    testfile.reset_index(drop=True, inplace=True)

    testfile.rename(columns={0: 'predicted_sales'}, inplace=True)

    # Assuming your DataFrame is named df and you want to round up values in the 'column_name' column
    testfile['predicted_sales'] = np.ceil(testfile['predicted_sales']).astype(int)

    # Combine the two datasets based on their index
    monthly_xgb_df = pd.concat([X_test, testfile], axis=1)

    # Group by 'year' and 'month' and sum 'predicted_sales'
    result_xgb_df = monthly_xgb_df.groupby(['year'], as_index=False)['predicted_sales'].sum()

    xgb_final = result_xgb_df.values.tolist()

    """Polynomial Regression"""

    import pickle

    with open('poly_storemodel.pkl', 'rb') as file:
        poly_model = pickle.load(file)

    poly = PolynomialFeatures(degree=2)
    poly_prediction = poly_model.predict(poly.fit_transform(X_test))

    poly_mape = mean_absolute_percentage_error(y_test, poly_prediction)
    poly_rmse = mean_squared_error(y_test, poly_prediction)
    poly_result = [poly_mape,poly_rmse]
    
    testfile = pd.DataFrame(poly_prediction)
    
    # Resetting the index of both DataFrames
    X_test.reset_index(drop=True, inplace=True)
    testfile.reset_index(drop=True, inplace=True)

    testfile.rename(columns={0: 'predicted_sales'}, inplace=True)

    # Assuming your DataFrame is named df and you want to round up values in the 'column_name' column
    testfile['predicted_sales'] = np.ceil(testfile['predicted_sales']).astype(int)

    # Combine the two datasets based on their index
    monthly_poly_df = pd.concat([X_test, testfile], axis=1)

    # Group by 'year' and 'month' and sum 'predicted_sales'
    result_poly_df = monthly_poly_df.groupby(['year'], as_index=False)['predicted_sales'].sum()

    poly_final = result_poly_df.values.tolist()

    """Original Values"""
    split_df['total_sales'] = np.ceil(split_df['total_sales']).astype(int)
    monthly_df = split_df.groupby(['year'], as_index=False)['total_sales'].sum()
    year_final = monthly_df.values.tolist()

    return [xgb_final,poly_final,year_final,xgb_result,poly_result]