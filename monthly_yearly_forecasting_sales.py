# -*- coding: utf-8 -*-
"""monthly_yearly_forecasting_sales

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mGAwVWj6mgAs014l5zge9Uppjy1CH_mN
"""

#Really need thesey
import pandas as pd
import numpy as np
from numpy import *

#Nice graphing tools
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.tools as tls

#Machine learning tools
from sklearn.model_selection import GridSearchCV,train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_percentage_error

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
# Fitting Linear Regression to the dataset
from sklearn.linear_model import LinearRegression

def test():
    # # Assuming your dataset is in a DataFrame named 'df' with columns 'Year', 'Month', 'Sales'
    df = pd.read_csv('C:/Users/amiru/OneDrive/Documents/VS code/Business Analytics Dashboard/Train/whole_sales.csv')
    df.head()

    #Checking the columns of the dataset
    df.columns

    df.info()

    df.describe()

    df.nunique().sort_values()

    """# Data Cleaning"""

    # Checking null values
    clean_df = df.copy()
    clean_df.isnull().sum()*100/clean_df.shape[0]

    #Removal of any Duplicate rows (if any)

    counter = 0
    rs,cs = clean_df.shape

    clean_df.drop_duplicates(inplace=True)

    if clean_df.shape==(rs,cs):
        print('\n\033[1mInference:\033[0m The dataset doesn\'t have any duplicates')
    else:
        print(f'\n\033[1mInference:\033[0m Number of duplicates dropped/fixed ---> {rs-clean_df.shape[0]}')

    # Formatting for sales_date and str_no

    clean_df['str_no'] = clean_df['str_no'].astype(str)

    clean_df['sales_date'] = pd.to_datetime(clean_df['sales_date'])

    print(clean_df['str_no'].dtypes,clean_df['sales_date'].dtypes)

    #TARGET ENCODING

    target_encoding = clean_df.groupby('str_no')['sales'].mean()
    clean_df['str_no'] = clean_df['str_no'].map(target_encoding)

    target_encoding = clean_df.groupby('cla_no')['sales'].mean()
    clean_df['cla_no'] = clean_df['cla_no'].map(target_encoding)

    target_encoding = clean_df.groupby('goo_no')['sales'].mean()
    clean_df['goo_no'] = clean_df['goo_no'].map(target_encoding)

    target_encoding = clean_df.groupby('sup_no')['sales'].mean()
    clean_df['sup_no'] = clean_df['sup_no'].map(target_encoding)

    clean_df.head()

    """# DATA EXPLORATION

    """

    exp_df = clean_df.copy()
    df.head()

    # FEATURE ENGINEERING
    # day/time features

    # Assuming 'sales_date' is in datetime format
    exp_df['day_of_week'] = exp_df['sales_date'].dt.dayofweek
    exp_df['month'] = exp_df['sales_date'].dt.month
    exp_df['quarter'] = exp_df['sales_date'].dt.quarter
    exp_df['year'] = exp_df['sales_date'].dt.year
    exp_df.head()

    """# DATA SPLITTING"""

    split_df = exp_df.copy()

    split_df.head()

    # Split the data into features (X) and target variable (y)
    X = split_df[['day_of_week','month','quarter','year','str_no','cla_no']]
    # 'sales_date','day_of_week','month','quarter','year'
    y = split_df['sales']
    # 'sales_date','day_of_week','month','quarter','year'
    # Split the data into training and testing sets (e.g., 80% train, 20% test)
    X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Display the shapes of the resulting sets
    print("X_train shape:", X_train.shape)
    print("X_dev shape:", X_dev.shape)
    print("y_train shape:", y_train.shape)
    print("y_dev shape:", y_dev.shape)

    """# MODEL PREDICTION

    XGBoost
    """

    import pickle

    with open('XGBoost_model.pkl', 'rb') as file:
        xgb_model = pickle.load(file)

    xgb_prediction = xgb_model.predict(X_dev)

    testfile = pd.DataFrame(xgb_prediction)

    # Resetting the index of both DataFrames
    X_dev.reset_index(drop=True, inplace=True)
    testfile.reset_index(drop=True, inplace=True)

    testfile.rename(columns={0: 'predicted_sales'}, inplace=True)

    # Assuming your DataFrame is named df and you want to round up values in the 'column_name' column
    testfile['predicted_sales'] = np.ceil(testfile['predicted_sales']).astype(int)

    # Combine the two datasets based on their index
    monthly_xgb_df = pd.concat([X_dev, testfile], axis=1)

    # Group by 'year' and 'month' and sum 'predicted_sales'
    result_xgb_df = monthly_xgb_df.groupby(['year', 'month'], as_index=False)['predicted_sales'].sum()
    
    xgb_final = result_xgb_df.values.tolist()

    """Polynomial Regression"""

    import pickle

    with open('poly_model.pkl', 'rb') as file:
        poly_model = pickle.load(file)

    poly = PolynomialFeatures(degree=2)
    poly_prediction = poly_model.predict(poly.fit_transform(X_dev))
    
    testfile = pd.DataFrame(poly_prediction)

    # Resetting the index of both DataFrames
    X_dev.reset_index(drop=True, inplace=True)
    testfile.reset_index(drop=True, inplace=True)

    testfile.rename(columns={0: 'predicted_sales'}, inplace=True)

    # Assuming your DataFrame is named df and you want to round up values in the 'column_name' column
    testfile['predicted_sales'] = np.ceil(testfile['predicted_sales']).astype(int)

    # Combine the two datasets based on their index
    monthly_poly_df = pd.concat([X_dev, testfile], axis=1)

    # Group by 'year' and 'month' and sum 'predicted_sales'
    result_poly_df = monthly_poly_df.groupby(['year', 'month'], as_index=False)['predicted_sales'].sum()

    poly_final = result_poly_df.values.tolist()

    return [xgb_final,poly_final]